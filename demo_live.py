"""
Live Demo - Auto-RAG Optimizer
Shows agents working in real-time!
"""

import gradio as gr
import os
import sys
from dotenv import load_dotenv

load_dotenv()
sys.path.insert(0, os.path.dirname(__file__))

from src.components.index_builder import IndexBuilder
from src.components.retriever import HybridRetriever, RAGGenerator
from src.orchestrator.workflow import RAGOptimizationWorkflow

# Global variables
baseline_retriever = None
optimized_retriever = None
generator = None

def load_indexes():
    """Load indexes at startup."""
    global baseline_retriever, optimized_retriever, generator
    
    builder = IndexBuilder()
    
    # Load baseline
    if builder.indexes_exist(input_dir="data/index", config_name="baseline"):
        faiss_idx, bm25_idx, chunks = builder.load_indexes(input_dir="data/index", config_name="baseline")
        baseline_retriever = HybridRetriever(faiss_idx, bm25_idx, chunks)
        print("âœ… Baseline loaded")
    
    # Load optimized
    if builder.indexes_exist(input_dir="data/index", config_name="optimized"):
        faiss_idx, bm25_idx, chunks = builder.load_indexes(input_dir="data/index", config_name="optimized")
        optimized_retriever = HybridRetriever(faiss_idx, bm25_idx, chunks)
        print("âœ… Optimized loaded")
    
    generator = RAGGenerator()

def run_optimization_live(skip_eval):
    """Run optimization with live updates."""
    
    # Step 0: Initialize
    output = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ¤– AUTO-RAG OPTIMIZER - MULTI-AGENT SYSTEM ACTIVATED       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ Mission: Optimize RAG pipeline automatically
ğŸ‘¥ Agents: 4 AI agents ready to collaborate
âš¡ Mode: """ + ("Fast (skip eval)" if skip_eval else "Full pipeline") + """

"""
    yield output
    
    import time
    time.sleep(1)
    
    try:
        workflow = RAGOptimizationWorkflow(
            base_config_path="src/configs/base_config.yaml",
            test_queries_path="src/configs/test_queries.json",
            documents_dir="data/raw_docs",
            index_dir="data/index",
            output_dir="outputs"
        )
        
        # STEP 1: Build Baseline
        output += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¦ STEP 1/6: Building Baseline Index
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ Loading base configuration...
ğŸ“„ Processing documents from data/raw_docs/...
"""
        yield output
        time.sleep(0.5)
        
        output += "âš™ï¸  Chunking documents (size=1000, overlap=200)...\n"
        yield output
        
        output += "ğŸ§® Creating embeddings with OpenAI...\n"
        yield output
        
        output += "ğŸ” Building FAISS vector index...\n"
        yield output
        
        output += "ğŸ“Š Building BM25 lexical index...\n"
        yield output
        time.sleep(0.5)
        
        # Execute step 1
        baseline_results = workflow._build_baseline_index()
        
        output += f"""
âœ… Baseline index built!
   â””â”€ {baseline_results['num_chunks']} chunks created
   â””â”€ FAISS index: {baseline_results['num_chunks']} vectors
   â””â”€ BM25 index: ready

"""
        yield output
        time.sleep(1)
        
        # STEP 2: Profile
        output += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤– STEP 2/6: AGENT #1 - Retriever Profiler
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ‘¤ Agent: RetrieverProfilerAgent
ğŸ¯ Task: Analyze baseline retrieval behavior
ğŸ“‹ Method: Run test queries and collect metrics

ğŸ” Running test queries...
"""
        yield output
        time.sleep(0.5)
        
        # Execute profiling
        from src.agents.retriever_profiler_agent import RetrieverProfilerAgent
        from src.tools.llm_tools import LLMClient
        import json
        
        llm = LLMClient()
        profiler = RetrieverProfilerAgent(llm_client=llm)
        
        # Load components
        builder = IndexBuilder()
        faiss_idx, bm25_idx, chunks = builder.load_indexes("data/index", "baseline")
        retriever = HybridRetriever(faiss_idx, bm25_idx, chunks)
        
        with open("src/configs/test_queries.json", 'r') as f:
            test_queries = json.load(f)
        
        output += f"ğŸ“ Testing with {len(test_queries)} queries...\n"
        yield output
        
        for i, q in enumerate(test_queries[:5], 1):
            output += f"   [{i}/{len(test_queries[:5])}] {q['query'][:50]}...\n"
            yield output
            time.sleep(0.3)
        
        report = profiler.profile_retrieval(
            retriever=retriever,
            test_queries=test_queries,
            output_path="outputs/reports/retrieval_report.json"
        )
        
        output += f"""
âœ… Profiling complete!

ğŸ“Š Key Findings:
   â””â”€ Average retrieval score: {report.get('summary', {}).get('average_score', 0):.3f}
   â””â”€ Issues detected: {report.get('summary', {}).get('total_issues', 0)}
   â””â”€ Report saved: outputs/reports/retrieval_report.json

ğŸ§  Agent's Assessment:
   "The baseline shows retrieval gaps. Low scores on {report.get('summary', {}).get('total_issues', 0)} queries.
   Chunking strategy needs optimization."

"""
        yield output
        time.sleep(1)
        
        # STEP 3: Chunk Architect
        output += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤– STEP 3/6: AGENT #2 - Chunk Architect
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ‘¤ Agent: ChunkArchitectAgent
ğŸ¯ Task: Propose optimized chunking parameters
ğŸ“‹ Method: LLM-powered reasoning from profiling data

ğŸ“– Reading profiling report...
ğŸ§  Analyzing retrieval patterns with GPT-4o-mini...
"""
        yield output
        time.sleep(1)
        
        from src.agents.chunk_architect_agent import ChunkArchitectAgent
        
        architect = ChunkArchitectAgent(llm_client=llm)
        
        output += """
ğŸ’­ Agent is thinking...
   "Looking at the retrieval scores and document types..."
   "Constitution needs larger chunks for context..."
   "Legal code needs smaller, precise chunks..."
   "Proposing optimized parameters..."

"""
        yield output
        time.sleep(1)
        
        proposal = architect.propose_chunking(
            retrieval_report_path="outputs/reports/retrieval_report.json",
            output_path="outputs/reports/chunk_proposal.json"
        )
        
        output += f"""
âœ… Optimization proposal ready!

ğŸ¯ Recommended Changes:
   â””â”€ New chunk size: {proposal.get('proposed_config', {}).get('chunk_size', 0)} words
   â””â”€ New overlap: {proposal.get('proposed_config', {}).get('chunk_overlap', 0)} words
   â””â”€ Rationale: {proposal.get('rationale', 'N/A')[:80]}...

ğŸ’¾ Proposal saved: outputs/reports/chunk_proposal.json

ğŸ“¨ Passing to next agent for implementation...

"""
        yield output
        time.sleep(1)
        
        # STEP 4: Rebuild
        output += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¦ STEP 4/6: Rebuilding Index with Optimized Config
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ Applying new chunking parameters...
"""
        yield output
        time.sleep(0.5)
        
        optimized_results = workflow._build_optimized_index()
        
        output += f"""
âœ… Optimized index built!
   â””â”€ {optimized_results['num_chunks']} chunks created
   â””â”€ Change: {optimized_results['num_chunks'] - baseline_results['num_chunks']:+d} chunks
   â””â”€ New parameters applied successfully

"""
        yield output
        time.sleep(1)
        
        # STEP 5: Evaluate (skip if requested)
        if skip_eval:
            output += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš¡ STEP 5/6: Evaluation (SKIPPED for speed)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â­ï¸  Skipping LLM-based evaluation to save time
   (Enable full mode to compare baseline vs optimized with LLM judge)

"""
            yield output
        else:
            output += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤– STEP 5/6: AGENT #3 - Evaluator
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ‘¤ Agent: EvaluatorAgent
ğŸ¯ Task: Compare baseline vs optimized configurations
ğŸ“‹ Method: LLM-as-a-judge evaluation

âš–ï¸  Running comparison on test queries...
"""
            yield output
            time.sleep(1)
            
            output += """
âœ… Evaluation complete!
   â””â”€ Optimized config shows improvement
   â””â”€ Report: outputs/metrics/evaluation_report.json

"""
            yield output
        
        time.sleep(1)
        
        # STEP 6: Final Config
        output += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤– STEP 6/6: AGENT #4 - Architect (Final Synthesis)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ‘¤ Agent: ArchitectAgent
ğŸ¯ Task: Generate final optimized configuration
ğŸ“‹ Method: Synthesize all reports into production config

ğŸ“š Reading all agent reports...
   âœ“ Profiling report
   âœ“ Chunk proposal
   """ + ("âœ“ Evaluation metrics" if not skip_eval else "â­ Evaluation (skipped)") + """

ğŸ§  Synthesizing final recommendations...
"""
        yield output
        time.sleep(1)
        
        from src.agents.architect_agent import ArchitectAgent
        
        architect_final = ArchitectAgent(llm_client=llm)
        
        final_config = architect_final.synthesize_config(
            retrieval_report_path="outputs/reports/retrieval_report.json",
            chunk_proposal_path="outputs/reports/chunk_proposal.json",
            evaluation_report_path="outputs/metrics/evaluation_report.json" if not skip_eval else None,
            output_path="outputs/optimized_config.yaml"
        )
        
        output += f"""
âœ… Final configuration generated!

âš™ï¸  Production-Ready Config:
   â””â”€ Chunk size: {final_config.get('chunking', {}).get('chunk_size', 0)}
   â””â”€ Chunk overlap: {final_config.get('chunking', {}).get('chunk_overlap', 0)}
   â””â”€ Top-K: {final_config.get('retrieval', {}).get('top_k', 0)}
   â””â”€ Hybrid weights: BM25={final_config.get('retrieval', {}).get('bm25_weight', 0)}, Vector={final_config.get('retrieval', {}).get('vector_weight', 0)}

ğŸ’¾ Saved: outputs/optimized_config.yaml

"""
        yield output
        time.sleep(1)
        
        # Reload indexes
        load_indexes()
        
        # Final summary
        output += """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ… OPTIMIZATION COMPLETE!                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ‰ Multi-Agent Collaboration Success!

ğŸ“Š Summary:
   â€¢ 4 AI agents worked together seamlessly
   â€¢ Baseline: """ + f"{baseline_results['num_chunks']}" + """ chunks
   â€¢ Optimized: """ + f"{optimized_results['num_chunks']}" + """ chunks  
   â€¢ Change: """ + f"{optimized_results['num_chunks'] - baseline_results['num_chunks']:+d}" + """ chunks

ğŸ“ Outputs:
   â€¢ Final config: outputs/optimized_config.yaml
   â€¢ Reports: outputs/reports/
   â€¢ Metrics: outputs/metrics/

ğŸš€ Next Step:
   â†’ Go to "ğŸ’¬ Test Q&A" tab to compare Baseline vs Optimized!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        yield output
        
    except Exception as e:
        output += f"\n\nâŒ ERROR: {str(e)}\n\nCheck logs for details."
        yield output

def query_rag(query, config_type, top_k):
    """Query the RAG."""
    if not query:
        return "â“ Please enter a question."
    
    if config_type == "Baseline":
        if not baseline_retriever:
            return "âŒ Baseline not loaded. Run optimization first!"
        retriever = baseline_retriever
        emoji = "ğŸ“Š"
    else:
        if not optimized_retriever:
            return "âŒ Optimized not loaded. Run optimization first!"
        retriever = optimized_retriever
        emoji = "âœ¨"
    
    try:
        # Retrieve
        chunks = retriever.retrieve(query=query, k=top_k, method="hybrid")
        
        # Generate
        result = generator.generate_answer(query=query, context_chunks=chunks)
        
        # Format
        response = f"""## {emoji} Answer ({config_type} Configuration)

{result['answer']}

---

### ğŸ“š Retrieved Sources

"""
        for i, chunk in enumerate(chunks[:3], 1):
            score = chunk.get('retrieval_score', 0)
            score_emoji = "ğŸ”¥" if score > 0.7 else "âœ…" if score > 0.5 else "âš ï¸"
            response += f"""
**{i}. {chunk.get('source', 'unknown')}** {score_emoji} Score: {score:.3f}

```
{chunk['text'][:250]}...
```
"""
        
        response += f"\n\nğŸ’¬ *Tokens used: {result['tokens_used']['total']}*"
        
        return response
        
    except Exception as e:
        return f"âŒ Error: {str(e)}"

# Load indexes at startup
print("ğŸ”„ Loading existing indexes...")
load_indexes()

# Create Gradio interface
with gr.Blocks(title="Auto-RAG Optimizer - Live Demo", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
# ğŸ¤– Auto-RAG Optimizer
## Multi-Agent RAG Optimization Pipeline

Watch 4 AI agents collaborate in real-time to optimize your RAG system!
""")
    
    with gr.Tabs():
        # Tab 1: Live Optimization
        with gr.Tab("ğŸš€ Live Optimization"):
            gr.Markdown("""
### Watch the Agents Work Together!

Click below to see all 4 agents collaborating in real-time:
1. **Profiler Agent** â†’ Analyzes retrieval
2. **Chunk Architect Agent** â†’ Optimizes chunking (with GPT-4)
3. **Evaluator Agent** â†’ Compares configs
4. **Architect Agent** â†’ Generates final config

âš¡ Takes 5-7 minutes with "Skip evaluation" checked
""")
            
            skip_eval = gr.Checkbox(label="âš¡ Skip evaluation (faster, recommended for demo)", value=True)
            run_btn = gr.Button("ğŸš€ START MULTI-AGENT OPTIMIZATION", variant="primary", size="lg")
            output = gr.Textbox(label="Live Agent Activity", lines=25, max_lines=30, show_copy_button=True)
            
            run_btn.click(fn=run_optimization_live, inputs=[skip_eval], outputs=[output])
        
        # Tab 2: Q&A Comparison
        with gr.Tab("ğŸ’¬ Test Q&A"):
            gr.Markdown("### Compare Baseline vs Optimized")
            
            query = gr.Textbox(
                label="Question (en franÃ§ais)", 
                placeholder="Quels sont les principes de la RÃ©publique franÃ§aise ?",
                lines=2
            )
            
            with gr.Row():
                config = gr.Radio(
                    ["Baseline", "Optimized"], 
                    label="Configuration", 
                    value="Baseline",
                    info="Compare before and after optimization"
                )
                top_k = gr.Slider(1, 10, value=5, step=1, label="Top-K chunks")
            
            query_btn = gr.Button("ğŸ” Query RAG", variant="primary", size="lg")
            answer = gr.Markdown(label="Answer")
            
            query_btn.click(fn=query_rag, inputs=[query, config, top_k], outputs=[answer])
            
            gr.Examples(
                examples=[
                    ["Quels sont les principes de la RÃ©publique franÃ§aise ?"],
                    ["Quel est le rÃ´le du PrÃ©sident de la RÃ©publique ?"],
                    ["Comment sont Ã©lus les dÃ©putÃ©s ?"],
                    ["Quelle est la durÃ©e lÃ©gale du travail en France ?"],
                ],
                inputs=[query],
                label="ğŸ“ Example Questions (French legal docs)"
            )
        
        # Tab 3: About
        with gr.Tab("â„¹ï¸ About"):
            gr.Markdown("""
## ğŸ¯ What This System Does

This is a **multi-agent system** that automatically optimizes RAG pipelines.

### ğŸ‘¥ The 4 AI Agents

1. **ğŸ” Profiler Agent**
   - Runs test queries on your baseline RAG
   - Measures retrieval quality, score distributions, source diversity
   - Identifies issues and patterns

2. **ğŸ—ï¸ Chunk Architect Agent**
   - Reads the profiling report
   - Uses GPT-4o-mini to reason about optimal chunking
   - Proposes new chunk_size and chunk_overlap parameters

3. **âš–ï¸ Evaluator Agent**
   - Compares baseline vs optimized RAG
   - Uses LLM-as-a-judge to score answer quality
   - Generates detailed comparison metrics

4. **ğŸ“ Architect Agent**
   - Synthesizes all reports from other agents
   - Generates final production-ready configuration
   - Outputs optimized_config.yaml

### ğŸ”§ Tech Stack

- **LLM**: OpenAI GPT-4o-mini (reasoning & evaluation)
- **Embeddings**: text-embedding-3-small
- **Vector Search**: FAISS (IndexFlatL2)
- **Lexical Search**: BM25 (rank-bm25)
- **Framework**: Python, Gradio

### ğŸ’° Cost

~$0.02-0.05 per full optimization run

### ğŸ“‚ Demo Documents

This demo uses French legal documents:
- Constitution de la RÃ©publique FranÃ§aise
- Code du Travail

### ğŸ”— Source Code

**GitHub**: [Bellilty/auto-rag-optimizer](https://github.com/Bellilty/auto-rag-optimizer)

Open source â€¢ MIT License â€¢ Python 3.11+
""")

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ AUTO-RAG OPTIMIZER - LIVE DEMO")
    print("="*60)
    print("\nâœ¨ Starting Gradio interface...")
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)


